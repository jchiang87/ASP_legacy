<?xml version="1.0" encoding="UTF-8"?>
<pipeline
   xmlns="http://glast-ground.slac.stanford.edu/pipeline"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://glast-ground.slac.stanford.edu/pipeline http://glast-ground.slac.stanford.edu/Pipeline-II/schemas/2.0/pipeline.xsd">
<!-- $Header$
-->
<task name="PGWave" version="9.0" type="Data">
  <variables>
    <!-- Default values that can be overridden at the command line. -->
    <var name="BINDIR">rh9_gcc32</var>
    <var name="datacatalog_imp">datacatalog</var>
    <var name="outputFolder">/ASP/Results/pgwave</var>
    <var name="xrootd_folder">/ASP/pgwave</var>
    <var name="GALPROP_MODEL">/afs/slac/g/glast/ground/ASP/GALPROP_MODEL/gll_iem_v02.fit</var>
    <var name="logRoot">/nfs/farm/g/glast/u52/ASP/log_files</var>
  </variables>
  <prerequisites>
    <prerequisite name="logicalPath" type="string"/>
    <prerequisite name="OUTPUTDIR" type="string"/>
    <prerequisite name="frequency" type="string"/>
    <prerequisite name="interval" type="integer"/>
    <prerequisite name="TSTART" type="integer"/>
    <prerequisite name="TSTOP" type="integer"/>
    <prerequisite name="ASP_PATH" type="string"/>
    <prerequisite name="CATDIR" type="string"/>
    <prerequisite name="ASP_PGWAVEROOT" type="string"/>
  </prerequisites>

  <process name="catalogQuery">
    <script>
      <![CDATA[
        print "using datacatalog implementation: ", datacatalog_imp
        datacat = eval(datacatalog_imp)
        def query(outfile, path, TSTART, TSTOP):
            output = pipeline.createFile(outfile)
            opt1 = '(%(TSTART)i <= nMetStart && nMetStop <= %(TSTOP)i)' % locals()
            opt2 = '(nMetStart <= %(TSTART)i && %(TSTART)i <= nMetStop)' % locals()
            opt3 = '(nMetStart <= %(TSTOP)i && %(TSTOP)i <= nMetStop)' % locals()
            query = ('DataType == \"%s\" && (%s || %s || %s)' 
                     % (path, opt1, opt2, opt3))
            datasetList = datacat.getDatasets(logicalPath+'/*', query)
            datasetList.writeToFile(output, 1)
            pipeline.writeFile(output)
        query("Ft1FileList", "FT1", TSTART, TSTOP)
        query("Ft2FileList", "FT2", TSTART, TSTOP)
        ]]></script>
  </process>

  <process name="getPgwInputData" autoRetryMaxAttempts="2">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/getPgwInputData.sh"
         batchOptions=" -q glastdataq" allocationGroup="glastdata"/>
    <depends>
      <after process="catalogQuery" status="SUCCESS"/>
    </depends>
  </process>

  <process name="runpgw" autoRetryMaxAttempts="2">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/runpgw.sh"
         batchOptions=" -q glastdataq" allocationGroup="glastdata"/>
    <depends>
      <after process="getPgwInputData" status="SUCCESS"/>
    </depends>
  </process>

  <process name="refinePositions" autoRetryMaxAttempts="2">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/refinePositions.sh"
         batchOptions=" -q glastdataq" allocationGroup="glastdata"/>
    <depends>
      <after process="runpgw" status="SUCCESS"/>
    </depends>
  </process>

  <process name="runsrcid" autoRetryMaxAttempts="2">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/runsrcid.sh"
         batchOptions=" -q glastdataq" allocationGroup="glastdata"/>
    <depends>
      <after process="refinePositions" status="SUCCESS"/>
    </depends>
  </process>

  <process name="createArchive" autoRetryMaxAttempts="1">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/asp_pgwave_createTarBall.sh"
         batchOptions=" -q glastdataq " allocationGroup="glastdata" />
    <depends>
      <after process="runsrcid" status="SUCCESS"/>
    </depends>
  </process>

  <process name="registerData">
    <script><![CDATA[
    print "using datacatalog implementation: ", datacatalog_imp
    datacat = eval(datacatalog_imp)
    streamId = pipeline.getStream()
    print "registering with stream ID = ", streamId
    #
    #  Register the archive file
    #
    createArchive = pipeline.getProcessInstance("createArchive")
    dataType = "PGW_ARCHIVE"

    filePath = createArchive.getVariable("tarball_name")

    attributes = (('nDatasetId=%s:nMetStart=%s:nMetStop=%s:' + 
                   'sFrequency=%s:nInterval=%i') %
                  (streamId, TSTART, TSTOP, frequency, interval))
    print attributes

    outfile = filePath.split('/')[-1]
    logicalPath = '%s/%s:%s' % (outputFolder, dataType, outfile)
    print logicalPath

    if filePath.find('root') == 0:
        filePath += "@SLAC_XROOT"
    print filePath

    datacatalog.registerDataset(dataType, logicalPath, filePath,
                                attributes)
    #
    # Register the ASP data viewer files (currently still stored on nfs)
    #
    def filename(ext, suffix):
        fn = 'Filtered_evt' + ext
        tokens = fn.split('.')
        return tokens[0] + '_' + suffix + '.' + tokens[1]
    suffix = OUTPUTDIR.split('/')[-1]
    exts = ('.fits', '_map.fits', '_map.list', '_map.reg', 
            '_map_pgw_out.fits', '_map_ait.gif', '_map_ait.png')
    files = [filename(ext, suffix) for ext in exts]
    dataTypes = ('EVENTS', 'SKYMAP', 'PGWAVESOURCELIST', 'DS9REGIONS', 
                 'PGWAVESUMMARY', 'SKYMAP', 'SKYMAP')
    for outfile, dataType in zip(files, dataTypes):
        logicalPath = ('%s/%s/%s:%s' 
                       % (outputFolder, frequency, dataType, outfile))
        print logicalPath
        filePath = '%s/%s' % (OUTPUTDIR, outfile)
        print filePath
        attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i' 
                      % (streamId, TSTART, TSTOP))
        print attributes
        datacat.registerDataset(dataType, logicalPath, 
                                filePath, attributes)
    # FT2 is a special case
    outfile = 'FT2_%s.fits' % suffix
    logicalPath = ('%s/%s/%s:%s' 
                   % (outputFolder, frequency, 'FT2', outfile))
    print logicalPath
    filePath = '%s/%s' % (OUTPUTDIR, outfile)
    print filePath
    attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i' 
                  % (streamId, TSTART, TSTOP))
    print attributes
    datacat.registerDataset('FT2', logicalPath, filePath, attributes)
    #
    if frequency in ('six_hours', 'daily', 'weekly'):
        pipeline.createSubstream("drpMonitoring", 0, 
                                 "pgwave_streamId=%i" % streamId)
    ]]></script>
    <depends>
      <after process="createArchive" status="SUCCESS"/>
    </depends>
    <createsSubtasks>
      <subtask>drpMonitoring</subtask>
    </createsSubtasks>
  </process>
  
  <process name="clean_up_files" autoRetryMaxAttempts="1">
    <job executable="${ASP_PGWAVEROOT}/${BINDIR}/asp_pgwave_clean_up_files.sh"
         batchOptions=" -q glastdataq " allocationGroup="glastdata" />
    <depends>
      <after process="registerData" status="SUCCESS"/>
    </depends>
  </process>

  <task name="drpMonitoring" version="1.0" type="Data">
    <prerequisites>
      <prerequisite name="pgwave_streamId" type="integer"/>
    </prerequisites>
    <process name="launchDrpMonitoring" autoRetryMaxAttempts="1">
      <job executable="${ASP_PGWAVEROOT}/${BINDIR}/launchDrpMonitoring.sh"
           batchOptions=" -q short"/>
    </process>
  </task><!--drpMonitoring-->

</task><!--PGWave-->
</pipeline>
