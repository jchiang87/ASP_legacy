<?xml version="1.0" encoding="UTF-8"?>
<pipeline
   xmlns="http://glast-ground.slac.stanford.edu/pipeline"
   xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xsi:schemaLocation="http://glast-ground.slac.stanford.edu/pipeline http://glast-ground.slac.stanford.edu/Pipeline-II/schemas/2.0/pipeline.xsd">
<!-- $Header$
-->
<task name="PGWave" version="6.2" type="Data">
  <variables>
    <!-- Default values that can be overridden at the command line. -->
    <var name="ST_INST">"/nfs/farm/g/glast/u30/builds/rh9_gcc32/ScienceTools/ScienceTools-v9r5p1"</var>
    <var name="BINDIR">rh9_gcc32</var>
    <var name="datacatalog_imp">datacatalog</var>
    <var name="outputFolder">/ASP/Results/pgwave</var>
  </variables>
  <prerequisites>
    <prerequisite name="logicalPath" type="string"/>
    <prerequisite name="OUTPUTDIR" type="string"/>
    <prerequisite name="frequency" type="string"/>
    <prerequisite name="interval" type="integer"/>
    <prerequisite name="TSTART" type="integer"/>
    <prerequisite name="TSTOP" type="integer"/>
    <prerequisite name="ASP_PATH" type="string"/>
    <prerequisite name="CATDIR" type="string"/>
    <prerequisite name="PGWAVEROOT" type="string"/>
  </prerequisites>

  <process name="catalogQuery">
    <script>
      <![CDATA[
        print "using datacatalog implementation: ", datacatalog_imp
        datacat = eval(datacatalog_imp)
        def query(outfile, path, TSTART, TSTOP):
            output = pipeline.createFile(outfile)
            opt1 = '(%(TSTART)i <= nMetStart && nMetStop <= %(TSTOP)i)' % locals()
            opt2 = '(nMetStart <= %(TSTART)i && %(TSTART)i <= nMetStop)' % locals()
            opt3 = '(nMetStart <= %(TSTOP)i && %(TSTOP)i <= nMetStop)' % locals()
            query = ('DataType == \"%s\" && (%s || %s || %s)' 
                     % (path, opt1, opt2, opt3))
            datasetList = datacat.getDatasets(logicalPath+'/*', query)
            datasetList.writeToFile(output, 1)
            pipeline.writeFile(output)
        query("Ft1FileList", "FT1", TSTART, TSTOP)
        query("Ft2FileList", "FT2", TSTART, TSTOP)
        ]]></script>
  </process>

  <process name="getPgwInputData">
    <job executable="${PGWAVEROOT}/${BINDIR}/getPgwInputData.sh"
         batchOptions=" -q short"/>
    <depends>
      <after process="catalogQuery" status="SUCCESS"/>
    </depends>
  </process>

  <process name="makeHealPixMaps">
    <job executable="${PGWAVEROOT}/${BINDIR}/makeHealPixMaps.sh"
         batchOptions="-q medium"/>
    <depends>
      <after process="getPgwInputData" status="SUCCESS"/>
    </depends>
  </process>

  <process name="runpgw">
    <job executable="${PGWAVEROOT}/${BINDIR}/runpgw.sh"
         batchOptions=" -q glastdataq" allocationGroup="glastdata"/>
    <depends>
      <after process="makeHealPixMaps" status="SUCCESS"/>
    </depends>
  </process>

  <process name="registerData">
    <script><![CDATA[
    print "using datacatalog implementation: ", datacatalog_imp
    datacat = eval(datacatalog_imp)
    def filename(ext, suffix):
        fn = 'Filtered_evt' + ext
        tokens = fn.split('.')
        return tokens[0] + '_' + suffix + '.' + tokens[1]
    suffix = OUTPUTDIR.split('/')[-1]
    exts = ('.fits', '_map.fits', '_map.list', '_map.reg', 
            '_map_pgw_out.fits', '_map_ait.gif')
    files = [filename(ext, suffix) for ext in exts]
    files.append('counts_%s.fits' % suffix)
    files.append('exposure_%s.fits' % suffix)
    dataTypes = ('EVENTS', 'SKYMAP', 'PGWAVESOURCELIST', 'DS9REGIONS', 
                 'PGWAVESUMMARY', 'SKYMAP', 'COUNTSMAP', 'EXPOSUREMAP')
    streamId = pipeline.getStream()
    print "registering with stream ID = ", streamId
    for outfile, dataType in zip(files, dataTypes):
        logicalPath = ('%s/%s/%s:%s' 
                       % (outputFolder, frequency, dataType, outfile))
        print logicalPath
        filePath = '%s/%s' % (OUTPUTDIR, outfile)
        print filePath
        #attributes = ('nProcessId=%s:nMetStart=%i:nMetStop=%i' 
        attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i' 
                      % (streamId, TSTART, TSTOP))
        print attributes
        datacat.registerDataset(dataType, logicalPath, 
                                filePath, attributes)
    # FT2 is a special case
    outfile = 'FT2_%s.fits' % suffix
    logicalPath = ('%s/%s/%s:%s' 
                   % (outputFolder, frequency, 'FT2', outfile))
    print logicalPath
    filePath = '%s/%s' % (OUTPUTDIR, outfile)
    print filePath
    attributes = ('nDatasetId=%s:nMetStart=%i:nMetStop=%i' 
                  % (streamId, TSTART, TSTOP))
    print attributes
    datacat.registerDataset('FT2', logicalPath, filePath, attributes)
    #
    if frequency in ('daily', 'weekly'):
        pipeline.createSubstream("drpMonitoring", 0, 
                                 "pgwave_streamId=%i" % streamId)
    ]]></script>
    <depends>
      <after process="runpgw" status="SUCCESS"/>
    </depends>
    <createsSubtasks>
      <subtask>drpMonitoring</subtask>
    </createsSubtasks>
  </process>
  
  <task name="drpMonitoring" version="1.0" type="Data">
    <prerequisites>
      <prerequisite name="pgwave_streamId" type="integer"/>
    </prerequisites>
    <process name="launchDrpMonitoring">
      <job executable="${PGWAVEROOT}/${BINDIR}/launchDrpMonitoring.sh"
           batchOptions=" -q short"/>
    </process>
  </task><!--drpMonitoring-->

</task><!--PGWave-->
</pipeline>
